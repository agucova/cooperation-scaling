# Scaling laws in language models

This is an (alpha quality) attempt at deriving scaling laws for cooperative behaviour in language models, adapting the setup from [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867), using [Pythia pretrained models](https://github.com/EleutherAI/pythia) to evaluate changes in behaviour at different parameter counts and training lengths.

![](plots/noisy/params_vs_training_steps_vs_efficiency_3d_scatterplot3.png)